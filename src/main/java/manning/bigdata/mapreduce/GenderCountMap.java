package manning.bigdata.mapreduce;


import com.backtype.hadoop.pail.PailStructure;
import com.backtype.hadoop.pail.SequenceFileFormat;
import manning.bigdata.ch3.DataPailStructure;
import manning.bigdata.swa.Data;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;

import java.io.IOException;

/**
 * User: ivaramme
 * Date: 7/25/14
 *
 * Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate
 * records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs.
 */
public class GenderCountMap extends MapReduceBase implements Mapper<Text, BytesWritable, Text, IntWritable> {
    private final static IntWritable ONE = new IntWritable(1);
    private Text name = new Text();
    private final PailStructure structure =  new DataPailStructure();
    private Data data;

    /**
     *
     * @param key
     * @param value the input value. Can be different from the output type
     * @param output collects mapped keys and values.
     * @param reporter reports status of the task
     * @throws IOException
     */
    public void map(Text key, BytesWritable value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
        String _name = "";
        try {
            // Deserialize raw data coming from HDFS using the structure
            data = (Data) structure.deserialize(value.getBytes());
            _name = data.getDataunit().getPerson_property().getProperty().getFull_name();;
        } catch (Exception e) {
            e.printStackTrace();
        }

        if(0 == _name.length())
            return;

        name.set(_name); // Set the String value to the 'TEXT' instance
        output.collect(name, ONE);
    }

    public static void main(String[] args) throws Exception {
        String hdfsURL;
        try {
            hdfsURL = System.getenv("HDFS_URL");
        } catch (Exception e) {
            throw new RuntimeException("Invalid hdfs Path");
        }

        JobConf conf = new JobConf(GenderCountMap.class);
        conf.setJobName("genderCount");
        conf.setOutputKeyClass(Text.class);
        conf.setOutputValueClass(IntWritable.class);

        conf.setMapperClass(GenderCountMap.class);
        //conf.setCombinerClass(Reduce.class);
        //conf.setReducerClass(Reduce.class);

        conf.setInputFormat(SequenceFileFormat.SequenceFilePailInputFormat.class);
        conf.setOutputFormat(TextOutputFormat.class);

        // Split data files
        // Using SequenceFileInputFormat as this is binary content
        // SequenceFileInputFormat
        SequenceFileFormat.SequenceFilePailInputFormat.setInputPathFilter(conf, PailFilter.class);
        SequenceFileFormat.SequenceFilePailInputFormat.setInputPaths(conf, new Path(hdfsURL + "/tmp/storm-test/201407252308")); // '*' is needed to go inside subdirectories
        FileOutputFormat.setOutputPath(conf, new Path(hdfsURL + "/tmp/output/"+System.currentTimeMillis()));

        JobClient.runJob(conf);

    }
}

// Filter to select the data files generated by pail
class PailFilter extends Configured implements PathFilter {

    @Override
    public boolean accept(Path path) {
        if(path.getName().endsWith("data"))
            return false;

        System.out.println(path);
        return true;
    }
}

/*
public class GenderCountReduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {

    }
}
*/